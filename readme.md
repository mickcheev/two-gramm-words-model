## Инструкция по установке

```
git clone https://github.com/mickcheev/two-gramm-words-model
cd two-gramm-words-model
```

## Детали реализации

 Программа может "кушать" только txt файлы для обучения. Намеренно убрана поддерка английского текста.
 (Больно хотелось загнать книгу "Чистый код" и не видеть отголоски legacy кода на джаве). Файлы должны иметь кодировку utf-8.

 Был реализован весь перечень команд из условия, как для обучения, так и для генерации. Модель сохраняется в формате pickle

 Мною был реализован простой алгоритм случайной выборки слова по префиксу используя веростность. Выбор суффикса для префикса
 производится из списка, где член списка это слово, возможного суффикса. Каждого слова в списке столько, сколько оно встречается после
 данного префикса. Это неоптимальный вариант например для английского языка, но учитывая склонения и падежи в русском, затраты памяти не
 столь существенные. При этом реализуется фактически вероятностная выборка (Статистическая веростность).

 Модель представляет собой набор связей между парами слов. То есть, была реализована 2-грамма. Я просто не успевал отладить триграмму, как мне посоветовали
 из поддержки :( . Не сочтите за оправдание

## Наборы для обучения

По умолчанию были добавлены следующие произведения:

Война и мир - Лев Толстой.
Финансист - Теодор Драйзер.
Титан - Теодор Драйзер.
Чистый код - Роберт Мартин.
Евгений Онегин - Александр Пушкин. (Было интересно, станут ли появлятся стихи)


## Проверяющему

Может быть, вы посчитаете, что я слишком наглый, но если у вас появится вопрос по коду, который вы захотите задать вот телега - @mikachami
Я старался писать понятные названия функций. Главный класс генерации, как вы и просили содержит только два метода. Второй правда широкий получился, но обработка текста всё же.
